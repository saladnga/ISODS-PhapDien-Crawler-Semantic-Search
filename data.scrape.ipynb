{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling the PhapDien Website\n",
    "\n",
    "## I. Crawling the PhapDien Website\n",
    "\n",
    "### Objective:\n",
    "Download and organize legal documents from the PhapDien website into structured directories for further processing.\n",
    "\n",
    "### Approach: [ISODS-PhapDien-Crawler-Semantic-Search](https://github.com/saladnga/ISODS-PhapDien-Crawler-Semantic-Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.12/site-packages (4.13.3)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.12/site-packages (5.3.0)\n",
      "Requirement already satisfied: selenium in ./.venv/lib/python3.12/site-packages (4.28.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: trio~=0.17 in ./.venv/lib/python3.12/site-packages (from selenium) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in ./.venv/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: websocket-client~=1.8 in ./.venv/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in ./.venv/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in ./.venv/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./.venv/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 lxml selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs and base directory\n",
    "full_base_url = \"https://vbpl.vn/TW/Pages/vbpq-toanvan.aspx?ItemID={}\"\n",
    "property_base_url = \"https://vbpl.vn/tw/Pages/vbpq-thuoctinh.aspx?&ItemID={}\"\n",
    "history_base_url = \"https://vbpl.vn/tw/Pages/vbpq-lichsu.aspx?&ItemID={}\"\n",
    "related_base_url = \"https://vbpl.vn/TW/Pages/vbpq-vanbanlienquan.aspx?ItemID={}\"\n",
    "pdf_base_url = \"https://vbpl.vn/tw/Pages/vbpq-van-ban-goc.aspx?ItemID={}\"\n",
    "base_dir = \"BoPhapDienDienTu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Additional Directories:\n",
    "- vbpl: For full text HTML documents\n",
    "- property: For property pages of the documents\n",
    "- history: For history pages of the documents\n",
    "- related: For related pages of the documents\n",
    "- pdf: For PDF files of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "demuc_dir = os.path.join(base_dir, \"demuc\")\n",
    "vbpl_dir = os.path.join(base_dir, \"vbpl\")\n",
    "property_dir = os.path.join(base_dir, \"property\")\n",
    "history_dir = os.path.join(base_dir, \"history\")\n",
    "related_dir = os.path.join(base_dir, \"related\")\n",
    "pdf_dir = os.path.join(base_dir, \"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl and Save HTML Documents:\n",
    "- Use os and BeautifulSoup to extract and iterate through unique ItemIDs from the index HTML files in the BoPhapDienDienTu/demuc directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all directories are created\n",
    "os.makedirs(vbpl_dir, exist_ok=True)\n",
    "os.makedirs(property_dir, exist_ok=True)\n",
    "os.makedirs(history_dir, exist_ok=True)\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "os.makedirs(related_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromeDriver to download PDF files\n",
    "chromedriver_path = \"chromedriver/chromedriver\"\n",
    "webdriver_pool = Queue()\n",
    "max_workers = 5\n",
    "webdriver_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver pool\n",
    "def init_webdriver():\n",
    "    for _ in range(max_workers):\n",
    "        options = Options()\n",
    "        service = Service(chromedriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        webdriver_pool.put(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a WebDriver instance\n",
    "def get_webdriver():\n",
    "    with webdriver_lock:\n",
    "        return webdriver_pool.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a WebDriver instance to the pool\n",
    "def return_webdriver(driver):\n",
    "    with webdriver_lock:\n",
    "        webdriver_pool.put(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract IDs from HTML files\n",
    "def extract_item_ids(demuc_dir):\n",
    "    item_ids = set()\n",
    "    for root, _, files in os.walk(demuc_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".html\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "                    for link in soup.find_all(\"a\", href=True):\n",
    "                        href = link[\"href\"]\n",
    "                        if \"ItemID=\" in href:\n",
    "                            item_id = href.split(\"ItemID=\")[1].split(\"&\")[0]\n",
    "                            item_ids.add(item_id)\n",
    "    return item_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each ItemIDs, construct their correspond URLs and save their content using request:\n",
    "    - For full text:\n",
    "        - URL: https://vbpl.vn/TW/Pages/vbpq-toanvan.aspx?ItemID=\n",
    "        - File path: full_.html\n",
    "        - Designated directory: BoPhapDienDienTu/vbpl\n",
    "    - For property:\n",
    "        - URL: https://vbpl.vn/tw/Pages/vbpq-thuoctinh.aspx?&ItemID=\n",
    "        - File path: p_.html\n",
    "        - Designated directory: BoPhapDienDienTu/property\n",
    "    - For history\n",
    "        - URL: https://vbpl.vn/tw/Pages/vbpq-lichsu.aspx?&ItemID=\n",
    "        - File path: h_.html\n",
    "        - Designated directory: BoPhapDienDienTu/history\n",
    "    - For related:\n",
    "        - URL: https://vbpl.vn/tw/Pages/vbpq-vanbanlienquan.aspx?&ItemID=\n",
    "        - File path: r_.html\n",
    "        - Designated directory: BoPhapDienDienTu/related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download HTML files from the URL\n",
    "def download_html(html_url, save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Skipping HTML file - already exists: {save_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(html_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"HTML file downloaded successfully - {save_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to download HTML file - {html_url} : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download PDF Files Dynamically:\n",
    "- Use Selenium and ChromDriver to extract and download PDF files.\n",
    "- Locate the PDF link in the data attribute of the tag using XPath\n",
    "    - For PDF:\n",
    "        - URL: https://vbpl.vn/tw/Pages/ vbpq-van-ban-goc?&ItemID=\n",
    "        - File path: pdf_.pdf\n",
    "        - Designated directory: BoPhapDienDienTu/pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the element for Selenium to download PDF files\n",
    "def find_element(driver, xpath):\n",
    "    try:\n",
    "        return driver.find_element(By.XPATH, xpath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding element by XPath: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF files from the URL using Selenium\n",
    "def download_pdf(url, save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"PDF file already exists: {save_path}\")\n",
    "        return\n",
    "\n",
    "    attempts = 3\n",
    "    for attempt in range(1, attempts + 1):\n",
    "        driver = None\n",
    "        try:\n",
    "            driver = get_webdriver()\n",
    "            driver.get(url)\n",
    "            time.sleep(10)\n",
    "\n",
    "            pdf_window = find_element(driver, \"//object\")\n",
    "            if pdf_window:\n",
    "                relative_pdf_url = pdf_window.get_attribute(\"data\")\n",
    "                if relative_pdf_url:\n",
    "                    if relative_pdf_url.startswith(\"https://vbpl.vn\") == False:\n",
    "                        pdf_url = f\"https://vbpl.vn{relative_pdf_url.lstrip('/')}\"\n",
    "                    else:\n",
    "                        pdf_url = relative_pdf_url\n",
    "\n",
    "                    print(f\"Found PDF URL at {pdf_url}\")\n",
    "\n",
    "                    response = requests.get(pdf_url, timeout=(30, 1200))\n",
    "                    if response.status_code == 200:\n",
    "                        with open(save_path, \"wb\") as file:\n",
    "                            file.write(response.content)\n",
    "                        print(f\"PDF downloaded successfully: {save_path}\")\n",
    "                        return\n",
    "                else:\n",
    "                    print(\"PDF URL not found on the page\")\n",
    "            else:\n",
    "                print(\"Cannot locate PDF\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                return_webdriver(driver)\n",
    "\n",
    "        time.sleep(5)\n",
    "    print(f\"Failed to download PDF after {attempts} attempts: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files with given ItemIDs\n",
    "def scrape_item(item_id):\n",
    "    urls = {\n",
    "        \"full_doc\": (full_base_url, \"full_{}.html\", vbpl_dir),\n",
    "        \"property\": (property_base_url, \"p_{}.html\", property_dir),\n",
    "        \"history\": (history_base_url, \"h_{}.html\", history_dir),\n",
    "        \"related\": (related_base_url, \"r_{}.html\", related_dir),\n",
    "        \"pdf\": (pdf_base_url, \"p_{}.pdf\", pdf_dir),\n",
    "    }\n",
    "\n",
    "    for type, (url, file_name, dir) in urls.items():\n",
    "        url = url.format(item_id)\n",
    "        file_name = file_name.format(item_id.split(\"#\")[0])\n",
    "        file_path = os.path.join(dir, file_name)\n",
    "\n",
    "        if type == \"pdf\":\n",
    "            download_pdf(url, file_path)\n",
    "        else:\n",
    "            download_html(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Downloads with Multiprocessing:\n",
    "- Use ThreadPoolExecutor from the concurrent.futures module download files concurrently, significantly speeding up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping HTML file - already exists: BoPhapDienDienTu/vbpl/full_.htmlSkipping HTML file - already exists: BoPhapDienDienTu/vbpl/full_133859.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/property/p_133859.html\n",
      "\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/vbpl/full_124052.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/vbpl/full_136705.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/history/h_133859.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/related/r_133859.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/property/p_136705.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/property/p_124052.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/vbpl/full_146048.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/history/h_136705.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/history/h_124052.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/property/p_146048.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/property/p_.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/related/r_136705.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/related/r_124052.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/history/h_146048.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/history/h_.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/related/r_146048.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/related/r_.html\n",
      "Error finding element by XPath: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=133.0.6943.55)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x00000001045b5bc8 chromedriver + 5766088\n",
      "1   chromedriver                        0x00000001045ad7ea chromedriver + 5732330\n",
      "2   chromedriver                        0x000000010409b680 chromedriver + 415360\n",
      "3   chromedriver                        0x000000010406f505 chromedriver + 234757\n",
      "4   chromedriver                        0x000000010411b5ee chromedriver + 939502\n",
      "5   chromedriver                        0x0000000104139d75 chromedriver + 1064309\n",
      "6   chromedriver                        0x0000000104112e23 chromedriver + 904739\n",
      "7   chromedriver                        0x00000001040debea chromedriver + 691178\n",
      "8   chromedriver                        0x00000001040dfd41 chromedriver + 695617\n",
      "9   chromedriver                        0x0000000104579100 chromedriver + 5517568\n",
      "10  chromedriver                        0x000000010457d040 chromedriver + 5533760\n",
      "11  chromedriver                        0x000000010455ac87 chromedriver + 5393543\n",
      "12  chromedriver                        0x000000010457dacb chromedriver + 5536459\n",
      "13  chromedriver                        0x0000000104549544 chromedriver + 5322052\n",
      "14  chromedriver                        0x000000010459b6e8 chromedriver + 5658344\n",
      "15  chromedriver                        0x000000010459b8af chromedriver + 5658799\n",
      "16  chromedriver                        0x00000001045ad3c8 chromedriver + 5731272\n",
      "17  libsystem_pthread.dylib             0x00007ff818e8d253 _pthread_start + 99\n",
      "18  libsystem_pthread.dylib             0x00007ff818e88bef thread_start + 15\n",
      "\n",
      "Cannot locate PDF\n",
      "Attempt 2 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Attempt 2 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Attempt 3 failed: HTTPConnectionPool(host='localhost', port=51356): Max retries exceeded with url: /session/248a2838e987798aacc21ff7f40adaef/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115c9e1e0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Attempt 3 failed: HTTPConnectionPool(host='localhost', port=51385): Max retries exceeded with url: /session/7b7e6799e0800d2f2ed8bcc72beb48ae/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115c9ebd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Attempt 3 failed: HTTPConnectionPool(host='localhost', port=51341): Max retries exceeded with url: /session/7829987541059ff65aadd21b967d27b2/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115c9f5f0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Error finding element by XPath: HTTPConnectionPool(host='localhost', port=51400): Max retries exceeded with url: /session/a1558ecc2464df098ab3ddc77331b0f5/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1116fb5f0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Cannot locate PDF\n",
      "Error finding element by XPath: HTTPConnectionPool(host='localhost', port=51370): Max retries exceeded with url: /session/a114bda1526a85079bf85767ff7e85ca/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115c9e8d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Cannot locate PDF\n",
      "Attempt 4 failed: HTTPConnectionPool(host='localhost', port=51356): Max retries exceeded with url: /session/248a2838e987798aacc21ff7f40adaef/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115c9f9e0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Attempt 4 failed: HTTPConnectionPool(host='localhost', port=51385): Max retries exceeded with url: /session/7b7e6799e0800d2f2ed8bcc72beb48ae/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115cf0470>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Attempt 4 failed: HTTPConnectionPool(host='localhost', port=51341): Max retries exceeded with url: /session/7829987541059ff65aadd21b967d27b2/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x110cefb90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Attempt 3 failed: HTTPConnectionPool(host='localhost', port=51400): Max retries exceeded with url: /session/a1558ecc2464df098ab3ddc77331b0f5/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115c9e210>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Attempt 3 failed: HTTPConnectionPool(host='localhost', port=51370): Max retries exceeded with url: /session/a114bda1526a85079bf85767ff7e85ca/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115c9f5c0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Failed to download PDF after 3 attempts: https://vbpl.vn/tw/Pages/vbpq-van-ban-goc.aspx?ItemID=136705#Chuong_II_Dieu_9\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/vbpl/full_86239.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/property/p_86239.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/history/h_86239.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/related/r_86239.html\n",
      "Attempt 2 failed: HTTPConnectionPool(host='localhost', port=51356): Max retries exceeded with url: /session/248a2838e987798aacc21ff7f40adaef/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x115c9ec30>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download PDF after 3 attempts: https://vbpl.vn/tw/Pages/vbpq-van-ban-goc.aspx?ItemID=146048#Chuong_V_Dieu_22\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/vbpl/full_117868.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/property/p_117868.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/history/h_117868.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/related/r_117868.html\n",
      "Failed to download PDF after 3 attempts: https://vbpl.vn/tw/Pages/vbpq-van-ban-goc.aspx?ItemID=133859#Chuong_II_Dieu_8\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/vbpl/full_136038.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/property/p_136038.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/history/h_136038.html\n",
      "Skipping HTML file - already exists: BoPhapDienDienTu/related/r_136038.html\n"
     ]
    }
   ],
   "source": [
    "# Execution (multiprocessing)\n",
    "full_item_ids = extract_item_ids(demuc_dir)\n",
    "init_webdriver()\n",
    "\n",
    "try:\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(scrape_item, item_id) for item_id in full_item_ids]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item: {e}\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exit\")\n",
    "finally:\n",
    "    while not webdriver_pool.empty():\n",
    "        driver = webdriver_pool.get()\n",
    "        driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
